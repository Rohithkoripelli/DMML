{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion to DB\n",
    "## Step 2 \n",
    "___The below code does the ingestion of data to a SQL DB. \n",
    "For every 2 minutes the data is ingested to DB. Total 4 batches of nearly 4K records are ingested to the DB\n",
    "here is logging done which clearly calls out the ingestion runs___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'logs/bank_db_ingestion_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read and split data \n",
    "df = pd.read_csv('/Users/rohithkoripelli/M.Tech/SEM2/DMML/Assignment/Churn Prediction/BankChurnFinal.csv')\n",
    "logging.info(f\"Read {len(df)} records from source file\")\n",
    "\n",
    "splits = []\n",
    "set1, set2, set3, set4 = np.array_split(df, 4)\n",
    "for dataset in [set1, set2, set3, set4]:\n",
    "    split1, split2 = np.array_split(dataset, 2)\n",
    "    splits.extend([split1, split2])\n",
    "logging.info(f\"Created {len(splits)} splits\")\n",
    "\n",
    "# Database connection with retry parameters\n",
    "conn_str = (\n",
    "    'DRIVER={ODBC Driver 18 for SQL Server};'\n",
    "    'SERVER=customerchurn.database.windows.net,1433;'\n",
    "    'DATABASE=customerchurn;'\n",
    "    'UID=sqladmin;'\n",
    "    'PWD=churn@123;'\n",
    "    'Encrypt=yes;'\n",
    "    'TrustServerCertificate=no;'\n",
    "    'Connection Timeout=600;'\n",
    "    'Command Timeout=60;'\n",
    "    'ConnectRetryCount=3;'\n",
    "    'ConnectRetryInterval=10'\n",
    ")\n",
    "\n",
    "engine = create_engine(\n",
    "    f'mssql+pyodbc:///?odbc_connect={quote_plus(conn_str)}',\n",
    "    pool_pre_ping=True,\n",
    "    pool_recycle=3600,\n",
    "    pool_timeout=60,\n",
    "    pool_size=5,\n",
    "    max_overflow=10\n",
    ")\n",
    "\n",
    "# Update create table SQL to match the actual table structure\n",
    "create_table_sql = \"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[dbo].[BankChurnData]') AND type in (N'U'))\n",
    "BEGIN\n",
    "    CREATE TABLE BankChurnData (\n",
    "        CustomerId BIGINT,              -- Primary key field\n",
    "        Surname VARCHAR(100),\n",
    "        CreditScore INT,\n",
    "        Geography VARCHAR(50),\n",
    "        Gender VARCHAR(10),\n",
    "        Age INT,\n",
    "        Tenure INT,\n",
    "        Balance DECIMAL(12,2),\n",
    "        NumOfProducts INT,\n",
    "        HasCrCard BIT,\n",
    "        IsActiveMember BIT,\n",
    "        EstimatedSalary DECIMAL(12,2),\n",
    "        Exited BIT,\n",
    "        ingestion_timestamp DATETIME\n",
    "    )\n",
    "END\n",
    "\"\"\"\n",
    "\n",
    "# First drop and recreate table to ensure correct schema\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS BankChurnData\"))\n",
    "        conn.execute(text(create_table_sql))\n",
    "        conn.commit()\n",
    "        logging.info(\"Table schema recreated without PRIMARY KEY constraint\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Table creation failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "    reraise=True\n",
    ")\n",
    "def insert_batch(engine, batch, batch_num):\n",
    "    with engine.connect() as conn:\n",
    "        before_count = conn.execute(text(\"SELECT COUNT(*) FROM BankChurnData\")).scalar()\n",
    "        batch.to_sql('BankChurnData', engine, if_exists='append', index=False, chunksize=50)\n",
    "        after_count = conn.execute(text(\"SELECT COUNT(*) FROM BankChurnData\")).scalar()\n",
    "        return after_count - before_count\n",
    "\n",
    "# Insert first 4 batches\n",
    "for i in range(4):\n",
    "    try:\n",
    "        batch = splits[i]\n",
    "        batch['ingestion_timestamp'] = datetime.now()\n",
    "        \n",
    "        # Convert data types\n",
    "        batch['CustomerId'] = batch['CustomerId'].astype('int64')\n",
    "        batch['CreditScore'] = batch['CreditScore'].astype('int32')\n",
    "        batch['Age'] = batch['Age'].astype('int32')\n",
    "        batch['Tenure'] = batch['Tenure'].astype('int32')\n",
    "        batch['Balance'] = pd.to_numeric(batch['Balance'], errors='coerce')\n",
    "        batch['NumOfProducts'] = batch['NumOfProducts'].astype('int32')\n",
    "        batch['HasCrCard'] = batch['HasCrCard'].astype('int32')\n",
    "        batch['IsActiveMember'] = batch['IsActiveMember'].astype('int32')\n",
    "        batch['EstimatedSalary'] = pd.to_numeric(batch['EstimatedSalary'], errors='coerce')\n",
    "        batch['Exited'] = batch['Exited'].astype('int32')\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        records_inserted = insert_batch(engine, batch, i+1)\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        logging.info(f\"Batch {i+1}: Inserted {records_inserted} records in {duration:.2f} seconds\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Batch {i+1} failed: {str(e)}\")\n",
    "\n",
    "logging.info(\"DB insertion completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion in to File share\n",
    "## Step 2 \n",
    "___The below code does the ingestion of data to a Azure file share. \n",
    "Total 4 batches of nearly 4K records are ingested to the Azure file share.\n",
    "There is logging done which clearly calls out the ingestion runs___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azure.storage.fileshare import ShareServiceClient\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'logs/fileshare_ingestion_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def cleanup_file_share(connection_string, share_name):\n",
    "    \"\"\"\n",
    "    Remove all files from the specified file share before fresh ingestion.\n",
    "    \"\"\"\n",
    "    service_client = ShareServiceClient.from_connection_string(conn_str=connection_string)\n",
    "    share_client = service_client.get_share_client(share_name)\n",
    "    for item in share_client.list_directories_and_files():\n",
    "        file_client = share_client.get_file_client(item.name)\n",
    "        file_client.delete_file()\n",
    "    logging.info(f\"Cleaned up old files in file share: {share_name}\")\n",
    "\n",
    "# Read and split data \n",
    "df = pd.read_csv('/Users/rohithkoripelli/M.Tech/SEM2/DMML/Assignment/Churn Prediction/BankChurnFinal.csv')\n",
    "logging.info(f\"Read {len(df)} records from source file\")\n",
    "\n",
    "splits = []\n",
    "set1, set2, set3, set4 = np.array_split(df, 4)\n",
    "for dataset in [set1, set2, set3, set4]:\n",
    "    split1, split2 = np.array_split(dataset, 2)\n",
    "    splits.extend([split1, split2])\n",
    "logging.info(f\"Created {len(splits)} splits\")\n",
    "\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=customerchurnsource;AccountKey=qxJurCvpimZ+qWV/AP3GNiMGrH7zFDYWF3B5SJ1/pxd5ppyXwiK2l4CeB7q3vKn5Wr5pZx4mJDE7+ASt0CDNcg==;EndpointSuffix=core.windows.net\"\n",
    "share_name = \"customerchurnsource\"\n",
    "\n",
    "# Clean up old files first\n",
    "cleanup_file_share(connection_string, share_name)\n",
    "# Track successful uploads\n",
    "successful_batches = set()\n",
    "\n",
    "# Process last 4 batches (indices 4-7)\n",
    "for i in range(4, 8):\n",
    "    batch_num = i + 1\n",
    "    \n",
    "    # Skip if already successful\n",
    "    if batch_num in successful_batches:\n",
    "        logging.info(f\"Batch {batch_num} already processed successfully, skipping\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        batch = splits[i]\n",
    "        batch_size = len(batch)\n",
    "        logging.info(f\"Processing batch {batch_num} with {batch_size} records\")\n",
    "        \n",
    "        # Add timestamp\n",
    "        batch['ingestion_timestamp'] = datetime.now()\n",
    "        \n",
    "        # Create unique filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'batch_{batch_num}_{timestamp}.csv'\n",
    "        temp_path = f'temp_{filename}'\n",
    "        \n",
    "        # Save to temp file\n",
    "        batch.to_csv(temp_path, index=False)\n",
    "        \n",
    "        # Upload to File Share\n",
    "        share_service_client = ShareServiceClient.from_connection_string(connection_string)\n",
    "        share_client = share_service_client.get_share_client(share_name)\n",
    "        file_client = share_client.get_file_client(filename)\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        with open(temp_path, 'rb') as file_data:\n",
    "            file_client.upload_file(file_data)\n",
    "        \n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Cleanup temp file\n",
    "        os.remove(temp_path)\n",
    "        \n",
    "        # Mark as successful\n",
    "        successful_batches.add(batch_num)\n",
    "        logging.info(f\"Batch {batch_num}: Uploaded {batch_size} records in {duration:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Batch {batch_num} failed: {str(e)}\")\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "logging.info(\"File Share ingestion completed\")\n",
    "logging.info(f\"Successfully processed batches: {sorted(list(successful_batches))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store ingested data in to Azure data lake\n",
    "## Step 3\n",
    "\n",
    "__In below code, Raw data from DB and file share is transfered to a Azure data lake blob container. Maintaing the right folder structure, type & timestamp.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.storage.fileshare import ShareServiceClient\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus\n",
    "import io\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'logs/blob_etl_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def fetch_from_db():\n",
    "    conn_str = (\n",
    "        'DRIVER={ODBC Driver 18 for SQL Server};'\n",
    "        'SERVER=customerchurn.database.windows.net,1433;'\n",
    "        'DATABASE=customerchurn;UID=sqladmin;PWD=churn@123;'\n",
    "        'Encrypt=yes;TrustServerCertificate=no;'\n",
    "    )\n",
    "    try:\n",
    "        engine = create_engine(f'mssql+pyodbc:///?odbc_connect={quote_plus(conn_str)}')\n",
    "        df = pd.read_sql(\"SELECT * FROM BankChurnData\", engine)\n",
    "        logging.info(f\"Fetched {len(df)} records from DB\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"DB fetch failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def cleanup_blob_container(conn_str, container_name):\n",
    "    \"\"\"\n",
    "    Delete and recreate the container with increased retry delay\n",
    "    \"\"\"\n",
    "    try:\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
    "        max_retries = 5  # Increased from 3\n",
    "        retry_delay = 15  # Increased from 5 seconds\n",
    "        \n",
    "        try:\n",
    "            container_client = blob_service_client.get_container_client(container_name)\n",
    "            if container_client.exists():\n",
    "                container_client.delete_container()\n",
    "                logging.info(f\"Deleted existing container: {container_name}\")\n",
    "                \n",
    "                # Longer initial wait after deletion\n",
    "                time.sleep(30)  # Added 30 second initial wait\n",
    "                \n",
    "                for attempt in range(max_retries):\n",
    "                    try:\n",
    "                        container_client = blob_service_client.create_container(container_name)\n",
    "                        logging.info(f\"Created new container: {container_name} on attempt {attempt + 1}\")\n",
    "                        return container_client\n",
    "                    except Exception as e:\n",
    "                        if attempt == max_retries - 1:\n",
    "                            raise\n",
    "                        logging.info(f\"Retrying container creation... Attempt {attempt + 1}\")\n",
    "                        time.sleep(retry_delay)\n",
    "        except:\n",
    "            container_client = blob_service_client.create_container(container_name)\n",
    "            logging.info(f\"Created new container: {container_name}\")\n",
    "            return container_client\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error cleaning up blob container: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def fetch_from_fileshare():\n",
    "    conn_str = \"DefaultEndpointsProtocol=https;AccountName=customerchurnsource;AccountKey=qxJurCvpimZ+qWV/AP3GNiMGrH7zFDYWF3B5SJ1/pxd5ppyXwiK2l4CeB7q3vKn5Wr5pZx4mJDE7+ASt0CDNcg==;EndpointSuffix=core.windows.net\"\n",
    "    try:\n",
    "        share_client = ShareServiceClient.from_connection_string(conn_str)\n",
    "        share = share_client.get_share_client(\"customerchurnsource\")\n",
    "        \n",
    "        dfs = []\n",
    "        for item in share.list_directories_and_files():\n",
    "            if item.name.endswith('.csv'):\n",
    "                file_client = share.get_file_client(item.name)\n",
    "                download_stream = file_client.download_file()\n",
    "                content = download_stream.readall()\n",
    "                df = pd.read_csv(pd.io.common.BytesIO(content))\n",
    "                dfs.append(df)\n",
    "                logging.info(f\"Read file: {item.name}\")\n",
    "        \n",
    "        if dfs:\n",
    "            final_df = pd.concat(dfs)\n",
    "            logging.info(f\"Fetched {len(final_df)} total records from File Share\")\n",
    "            return final_df\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"File Share fetch failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def write_to_blob(df, source):\n",
    "    try:\n",
    "        conn_str = \"DefaultEndpointsProtocol=https;AccountName=churndest;AccountKey=y8ZXlGbsdIX6Z7sP0/2VMxL9+7NKTLv4M6fvbhczK8T+fP/LJXfuvturAeghXm+89QAmosxQS8+/+AStywM6UQ==;EndpointSuffix=core.windows.net\"\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
    "        \n",
    "        container_name = \"customerchurn\"\n",
    "        try:\n",
    "            container_client = blob_service_client.create_container(container_name)\n",
    "            logging.info(f\"Created new container: {container_name}\")\n",
    "        except:\n",
    "            container_client = blob_service_client.get_container_client(container_name)\n",
    "            logging.info(f\"Using existing container: {container_name}\")\n",
    "\n",
    "        now = datetime.now()\n",
    "        path = f\"source={source}/year={now.year}/month={now.month:02d}/day={now.day:02d}/hour={now.hour:02d}\"\n",
    "        filename = f\"data_{now.strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        blob_path = f\"{path}/{filename}\"\n",
    "\n",
    "        # Convert to CSV instead of parquet\n",
    "        csv_buffer = io.StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        csv_buffer.seek(0)\n",
    "\n",
    "        blob_client = container_client.get_blob_client(blob_path)\n",
    "        blob_client.upload_blob(csv_buffer.getvalue(), overwrite=True)\n",
    "        \n",
    "        logging.info(f\"Successfully written {len(df)} records to {blob_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Blob storage write failed for {source}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Use the actual connection string\n",
    "        conn_str = \"DefaultEndpointsProtocol=https;AccountName=churndest;AccountKey=y8ZXlGbsdIX6Z7sP0/2VMxL9+7NKTLv4M6fvbhczK8T+fP/LJXfuvturAeghXm+89QAmosxQS8+/+AStywM6UQ==;EndpointSuffix=core.windows.net\"\n",
    "        container_name = \"customerchurn\"\n",
    "        \n",
    "        # Clean up and get container client\n",
    "        container_client = cleanup_blob_container(conn_str, container_name)\n",
    "        \n",
    "        # Process DB data\n",
    "        db_data = fetch_from_db()\n",
    "        if db_data is not None:\n",
    "            write_to_blob(db_data, \"db\")\n",
    "        \n",
    "        # Process File Share data\n",
    "        fs_data = fetch_from_fileshare()\n",
    "        if fs_data is not None:\n",
    "            write_to_blob(fs_data, \"fileshare\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Main process failed: {str(e)}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data validation\n",
    "## Step 4\n",
    "___Raw data from Azure data lake is fetched for completing data validation___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 13:53:34,290 - INFO - Request URL: 'https://churndest.blob.core.windows.net/customerchurn?restype=REDACTED&comp=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/xml'\n",
      "    'User-Agent': 'azsdk-python-storage-blob/12.24.1 Python/3.9.6 (macOS-15.1-arm64-arm-64bit)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': '4e96e372-f0f6-11ef-aae8-6241a593cdd7'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "2025-02-22 13:53:35,646 - INFO - Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/xml'\n",
      "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': 'c92100e9-301e-00a2-5d03-855a8d000000'\n",
      "    'x-ms-client-request-id': '4e96e372-f0f6-11ef-aae8-6241a593cdd7'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Date': 'Sat, 22 Feb 2025 08:23:34 GMT'\n",
      "2025-02-22 13:53:35,649 - INFO - Request URL: 'https://churndest.blob.core.windows.net/customerchurn/source%3Ddb/year%3D2025/month%3D02/day%3D22/hour%3D13/data_20250222_135231.csv'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-range': 'REDACTED'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/xml'\n",
      "    'User-Agent': 'azsdk-python-storage-blob/12.24.1 Python/3.9.6 (macOS-15.1-arm64-arm-64bit)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': '4f6643c4-f0f6-11ef-aae8-6241a593cdd7'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "2025-02-22 13:53:35,896 - INFO - Response status: 206\n",
      "Response headers:\n",
      "    'Content-Length': '1607680'\n",
      "    'Content-Type': 'application/octet-stream'\n",
      "    'Content-Range': 'REDACTED'\n",
      "    'Last-Modified': 'Sat, 22 Feb 2025 08:22:32 GMT'\n",
      "    'Accept-Ranges': 'REDACTED'\n",
      "    'ETag': '\"0x8DD531A0E3C8FFC\"'\n",
      "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': 'c921014f-301e-00a2-3f03-855a8d000000'\n",
      "    'x-ms-client-request-id': '4f6643c4-f0f6-11ef-aae8-6241a593cdd7'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-resource-type': 'REDACTED'\n",
      "    'x-ms-creation-time': 'REDACTED'\n",
      "    'x-ms-blob-content-md5': 'REDACTED'\n",
      "    'x-ms-lease-status': 'REDACTED'\n",
      "    'x-ms-lease-state': 'REDACTED'\n",
      "    'x-ms-blob-type': 'REDACTED'\n",
      "    'x-ms-server-encrypted': 'REDACTED'\n",
      "    'x-ms-owner': 'REDACTED'\n",
      "    'x-ms-group': 'REDACTED'\n",
      "    'x-ms-permissions': 'REDACTED'\n",
      "    'x-ms-acl': 'REDACTED'\n",
      "    'Date': 'Sat, 22 Feb 2025 08:23:35 GMT'\n",
      "2025-02-22 13:53:37,727 - INFO - Read: source=db/year=2025/month=02/day=22/hour=13/data_20250222_135231.csv\n",
      "2025-02-22 13:53:37,729 - INFO - Request URL: 'https://churndest.blob.core.windows.net/customerchurn/source%3Dfileshare/year%3D2025/month%3D02/day%3D22/hour%3D13/data_20250222_135237.csv'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-range': 'REDACTED'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/xml'\n",
      "    'User-Agent': 'azsdk-python-storage-blob/12.24.1 Python/3.9.6 (macOS-15.1-arm64-arm-64bit)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': '50a38170-f0f6-11ef-aae8-6241a593cdd7'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "2025-02-22 13:53:37,968 - INFO - Response status: 206\n",
      "Response headers:\n",
      "    'Content-Length': '1479627'\n",
      "    'Content-Type': 'application/octet-stream'\n",
      "    'Content-Range': 'REDACTED'\n",
      "    'Last-Modified': 'Sat, 22 Feb 2025 08:22:45 GMT'\n",
      "    'Accept-Ranges': 'REDACTED'\n",
      "    'ETag': '\"0x8DD531A15C184C8\"'\n",
      "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': 'c92102b0-301e-00a2-1503-855a8d000000'\n",
      "    'x-ms-client-request-id': '50a38170-f0f6-11ef-aae8-6241a593cdd7'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-resource-type': 'REDACTED'\n",
      "    'x-ms-creation-time': 'REDACTED'\n",
      "    'x-ms-blob-content-md5': 'REDACTED'\n",
      "    'x-ms-lease-status': 'REDACTED'\n",
      "    'x-ms-lease-state': 'REDACTED'\n",
      "    'x-ms-blob-type': 'REDACTED'\n",
      "    'x-ms-server-encrypted': 'REDACTED'\n",
      "    'x-ms-owner': 'REDACTED'\n",
      "    'x-ms-group': 'REDACTED'\n",
      "    'x-ms-permissions': 'REDACTED'\n",
      "    'x-ms-acl': 'REDACTED'\n",
      "    'Date': 'Sat, 22 Feb 2025 08:23:37 GMT'\n",
      "2025-02-22 13:53:38,458 - INFO - Read: source=fileshare/year=2025/month=02/day=22/hour=13/data_20250222_135237.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Shape: (32842, 14)\n",
      "\n",
      "Missing Values:\n",
      "CustomerId               0\n",
      "Surname                  0\n",
      "CreditScore              0\n",
      "Geography                0\n",
      "Gender                   0\n",
      "Age                      0\n",
      "Tenure                   0\n",
      "Balance                357\n",
      "NumOfProducts            0\n",
      "HasCrCard                0\n",
      "IsActiveMember           0\n",
      "EstimatedSalary        346\n",
      "Exited                   0\n",
      "ingestion_timestamp      0\n",
      "dtype: int64\n",
      "\n",
      "Data Description:\n",
      "         CustomerId   CreditScore           Age        Tenure        Balance  \\\n",
      "count  3.284200e+04  32842.000000  32842.000000  32842.000000   32485.000000   \n",
      "mean   1.551574e+07    645.094879     38.493271      4.957798   75984.918590   \n",
      "std    2.255006e+06    134.471892     11.726820      2.933764   63022.562214   \n",
      "min    0.000000e+00      0.000000      0.000000      0.000000       0.000000   \n",
      "25%    1.457840e+07    566.000000     31.000000      2.000000       0.000000   \n",
      "50%    1.566222e+07    647.000000     37.000000      5.000000   93964.320000   \n",
      "75%    1.673633e+07    729.000000     44.000000      7.000000  126943.940000   \n",
      "max    2.257508e+07   1121.000000    121.000000     13.000000  269363.560000   \n",
      "\n",
      "       NumOfProducts     HasCrCard  IsActiveMember  EstimatedSalary  \\\n",
      "count   32842.000000  32842.000000    32842.000000     32496.000000   \n",
      "mean        1.513915      0.702546        0.513489    100570.408904   \n",
      "std         0.601910      0.457145        0.499826     58501.998980   \n",
      "min         0.000000      0.000000        0.000000        11.580000   \n",
      "25%         1.000000      0.000000        0.000000     51356.957500   \n",
      "50%         1.000000      1.000000        1.000000     99343.605000   \n",
      "75%         2.000000      1.000000        1.000000    148442.462500   \n",
      "max         5.000000      1.000000        1.000000    262074.450000   \n",
      "\n",
      "             Exited  \n",
      "count  32842.000000  \n",
      "mean       0.200962  \n",
      "std        0.400726  \n",
      "min        0.000000  \n",
      "25%        0.000000  \n",
      "50%        0.000000  \n",
      "75%        0.000000  \n",
      "max        1.000000  \n",
      "\n",
      "Duplicate Records: 500\n",
      "\n",
      "CreditScore Range: 0 1121\n",
      "Age Range: 0 121\n",
      "Tenure Range: 0 13\n",
      "Balance Range: 0.0 269363.56\n",
      "NumOfProducts Range: 0 5\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def fetch_and_analyze():\n",
    "    # Connect to blob\n",
    "    conn_str = \"DefaultEndpointsProtocol=https;AccountName=churndest;AccountKey=y8ZXlGbsdIX6Z7sP0/2VMxL9+7NKTLv4M6fvbhczK8T+fP/LJXfuvturAeghXm+89QAmosxQS8+/+AStywM6UQ==;EndpointSuffix=core.windows.net\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
    "    container_client = blob_service_client.get_container_client(\"customerchurn\")\n",
    "\n",
    "    # Read all CSVs\n",
    "    dfs = []\n",
    "    for blob in container_client.list_blobs():\n",
    "        if blob.name.endswith('.csv'):\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            content = blob_client.download_blob().readall()\n",
    "            df = pd.read_csv(io.BytesIO(content))\n",
    "            dfs.append(df)\n",
    "            logging.info(f\"Read: {blob.name}\")\n",
    "\n",
    "    # Combine and analyze\n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        print(\"\\nData Shape:\", combined_df.shape)\n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(combined_df.isnull().sum())\n",
    "        print(\"\\nData Description:\")\n",
    "        print(combined_df.describe())\n",
    "        # Print duplicate records\n",
    "        print(\"\\nDuplicate Records:\", combined_df.duplicated().sum())\n",
    "        # Validate range of columns\n",
    "        print(\"\\nCreditScore Range:\", combined_df['CreditScore'].min(), combined_df['CreditScore'].max())\n",
    "        print(\"Age Range:\", combined_df['Age'].min(), combined_df['Age'].max())\n",
    "        print(\"Tenure Range:\", combined_df['Tenure'].min(), combined_df['Tenure'].max())\n",
    "        print(\"Balance Range:\", combined_df['Balance'].min(), combined_df['Balance'].max())\n",
    "        print(\"NumOfProducts Range:\", combined_df['NumOfProducts'].min(), combined_df['NumOfProducts'].max())\n",
    "        \n",
    "    else:\n",
    "        print(\"No CSV files found\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_and_analyze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
